package generator

import java.lang
import java.io.{BufferedWriter, File, FileWriter}

import org.apache.flink.api.common.functions.RichMapPartitionFunction
import org.apache.flink.api.common.typeinfo.TypeInformation
import org.apache.flink.api.scala._
import org.apache.flink.table.api.Table
import org.apache.flink.table.api.scala._
import org.apache.flink.api.scala.utils._
import org.apache.flink.table.api.scala.BatchTableEnvironment
import org.apache.flink.util.Collector
import org.scalacheck.{Arbitrary, Gen}
import org.scalacheck.Gen.Parameters
import org.scalacheck.rng.Seed
import utilities.FilesPath

import scala.reflect.ClassTag

/** Object creator of DataSet an Table API */
object Generator {

  /**
   * RichMapPartitionFunction used for testing generator's fault tolerance. When flink fails, the function writes in a temp file
   * the data each partition has generated in /tmp folder
   * @tparam A Type of DataSet generated
   */
  class FaultTolerantSeeds[A] extends RichMapPartitionFunction[A, Int] {
    private val extension = ".txt"

    /**
     * Writes in a file for each partition every element separated by a comma, and them throw a exception to make flink fail
     * @param values values generated for a partition
     * @param out not important due to the program throws an exception
     */
    override def mapPartition(values: lang.Iterable[A], out: Collector[Int]): Unit = {
      val attempt = getRuntimeContext.getAttemptNumber
      val task = getRuntimeContext.getIndexOfThisSubtask
      var ownElements = List.empty[String]
      val currentFile = "partition_" + task + "_attempt_" + attempt + "-"
      val f = File.createTempFile(currentFile, extension)

      FilesPath.setMatrixFilePath(task, attempt, f.getAbsolutePath) //Add path to access to it later
      val writer = new BufferedWriter(new FileWriter(f))

      values.forEach({
        xs =>
          ownElements =  xs.toString :: ownElements
      })


      for (index <- ownElements.indices) {

        if (index != ownElements.size-1) {
          writer.write(ownElements(index) + ", ")
        }else{
          writer.write(ownElements(index))
        }
      }
      writer.flush()
      writer.close()

      Thread.sleep(1000)

      throw new Exception("Testing fault tolerance")

    }
  }

  /**
   * Creates a ScalaCheck Gen contained by Apache Flink DataSets of the required type. It load balances the data among
   * the partitions specified by parameter, make it scalable
   * @param numElements Number of A elements generated by each partition
   * @param numPartitions Number of Apache Flink partitions using in the process
   * @param g Generator used to generate data of type A
   * @param seedOpt Seed passed by parameter to reproduce a specific Gen which will return the same values. This makes the
   *                generator fault-tolerant
   * @param env implicit Apache Flink environment
   * @tparam A type of elements created for DataSet
   * @return A DataSet Gen of A values
   */
  def generateDataSetGenerator[A: ClassTag : TypeInformation](numElements: Int, numPartitions: Int, g: Gen[A], seedOpt: Option[Int] = None)
                                                             (implicit env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment): Gen[DataSet[A]] = {

    val seedGen: Gen[Int] = if (seedOpt.isDefined) Gen.const(seedOpt.get) else Arbitrary.arbitrary[Int]

    for {
      seed <- seedGen
      seeds = Gen.listOfN(numPartitions, Arbitrary.arbitrary[Int]).apply(Parameters.default, Seed.apply(seed)).get
    } yield
      env.fromCollection(seeds)
      .rebalance() //Make a load balanced strategy scattering a number of lists equals to numPartitions, among the available task slots
      .flatMap { xs =>
        val elements: List[A] = Gen.listOfN(numElements, g).apply(Parameters.default, Seed.apply(xs)).getOrElse(Nil)
        elements
      }
      .setParallelism(numPartitions)
  }


  /**
   * Creates a ScalaCheck Gen contained by Apache Flink Tables of the required type. It load balances the data among
   * the partitions specified by parameter, make it scalable. Table is built from getting the DataSet Gen
   * @param numElements Number of A elements generated by each partition
   * @param numPartitions Number of Apache Flink partitions using in the process
   * @param g Generator used to generate data of type A
   * @param seedOpt Seed passed by parameter to reproduce a specific Gen which will return the same values. This makes the
   *                generator fault-tolerant
   * @param auto_increment true if an auto incremental id is needed. It is placed as the first field of the table
   * @param tEnv implicit Apache Flink table environment
   * @param env implicit Apache Flink environment
   * @tparam A type of elements created for DataSet
   * @return Table Gen of A values
   */
  def generateDataSetTableGenerator[A: ClassTag : TypeInformation](numElements: Int, numPartitions: Int, g: Gen[A], seedOpt: Option[Int] = None, auto_increment: Boolean = false)
                                                             (implicit tEnv: BatchTableEnvironment, env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment): Gen[Table] = {

    for {
      dataSet <- generateDataSetGenerator(numElements, numPartitions, g, seedOpt)
    } yield
      if(auto_increment) tEnv.fromDataSet(dataSet.zipWithIndex, '_1, '_2).orderBy('_1) else tEnv.fromDataSet(dataSet)
    //If A is primitive type, it is needed to access with 'f0 to operate. If not it has its vals names. e.g: Person -> val name and val age ==> 'name and 'age
  }
}

